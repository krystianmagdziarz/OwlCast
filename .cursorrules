# Product Requirements Document

## Project Overview

We are building a service that will collect statistics from websites. The service will be used to track the number of views and other statistics for each page on a website.
Statistics will be collected from multiple websites and stored in a clickhouse database. The service will be used to retrieve and analyze the statistics. 
Javascript is used to collect the statistics from the websites and send them to the service (using a POST request), but the service will be designed to be stateless and scalable.


## Business value

Main goal is to provide a statistic service for users to use with their websites using REST API.
Users will be able to render its own dashboard using the collected data, and use it to make data-driven decisions.
This will help users to integrate statistics with their own applications.


## Constraints

* It should be able to handle a large number of websites and pages, and be able to scale horizontally to handle the load. 
* It should be designed to be secure and to protect the privacy of the websites.
* It should be designed to be flexible and easy to extend with new features.
* It should be designed to be easy to understand and maintain.
* It should be designed to be easy to deploy and scale.
* It should be designed to be easy to monitor and troubleshoot.
* It should be designed to be easy to backup and restore.
* It should be designed to be easy to integrate with other services.
* It should be designed to be easy to use and to be self-documenting.
* It should work with cached pages (e.g. pages that are cached in the browser), cloudflare caching, static web pages, etc.

## Core functionality

* Save the statistics for a page.
* Retrieve the statistics for a page.
* Retrieve the statistics for all pages.
* Retrieve the statistics for a domain.
* Retrieve the statistics for all domains.
* Retrieve the statistic in a time range and in a time period (e.g. last 24 hours, last 7 days, last 30 days, etc.).
* Collect the statistic in time intervals (e.g. every 10 seconds, every 1 minute, every 1 hour, etc.).

## Collecting statistics metrics

* Number of views
* Number of unique visitors
* Bounce rate
* Average time on page
* Percentage scroll depth
* Mobile / desktop
* Referrer
* User agent
* Screen size
* Language
* Country
* City
* ISP

## Tech stack

* FastAPI
* Clickhouse
* Redis
* Celery
* Docker Compose
* Nginx

## File structure

ðŸ“¦ statcollector
â”œâ”€â”€ ðŸ“‚ app # Main application package
â”‚   â”œâ”€â”€ ðŸ“‚ api # API endpoints and routing
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ v1 # API version 1 implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ statistics.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ health.py
â”‚   â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ ðŸ“‚ core # Core functionality (config, security, events)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â””â”€â”€ events.py
â”‚   â”œâ”€â”€ ðŸ“‚ db # Database connections and models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ clickhouse.py
â”‚   â”‚   â””â”€â”€ redis.py
â”‚   â”œâ”€â”€ ðŸ“‚ models # Database models and table definitions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ statistics.py
â”‚   â”œâ”€â”€ ðŸ“‚ schemas # Pydantic models for request/response validation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ statistics.py
â”‚   â”œâ”€â”€ ðŸ“‚ services # Business logic layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ statistics.py
â”‚   â”œâ”€â”€ ðŸ“‚ utils # Utility functions and helpers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ helpers.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ ðŸ“‚ client # JavaScript/TypeScript client implementation
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ src # Client source code
â”‚   â”‚   â”œâ”€â”€ collector.ts
â”‚   â”‚   â””â”€â”€ types.ts
â”‚   â””â”€â”€ tsconfig.json
â”œâ”€â”€ ðŸ“‚ tests # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_api # API endpoint tests
â”‚   â”‚   â””â”€â”€ test_statistics.py
â”‚   â””â”€â”€ test_services # Business logic tests
â”‚       â””â”€â”€ test_statistics.py
â”œâ”€â”€ ðŸ“‚ docs # Project documentation
â”‚   â”œâ”€â”€ index.md
â”‚   â””â”€â”€ api.md
â”œâ”€â”€ ðŸ“‚ compose # Docker compose configurations
â”‚   â”œâ”€â”€ local # Local development setup
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ entrypoint.sh
â”‚   â””â”€â”€ production # Production deployment setup
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ entrypoint.sh
â”œâ”€â”€ ðŸ“‚ scripts # Utility scripts for development
â”‚   â”œâ”€â”€ lint.sh
â”‚   â””â”€â”€ test.sh
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ local.yml
â”œâ”€â”€ production.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

Use best practices for the file structure of fastapi project.


## API

* Serve the statistics data using a REST API.
* Use async code to serve the statistics data.
* Use clickhouse to store the statistics data.
* Serve endpoints to visualize the statistics data (e.g. number of views per day, number of views per page, etc.).
* Make sure to validate the input data to avoid storing incorrect data.
* Use pydantic to validate the input data.
* Use fastapi-utils to add extra functionality to the endpoints.
* Use fastapi-cache to cache the results.
* Use fastapi-limiter to limit the number of requests.
* Make endpoints for collecting the statistics data for charts.

## Writing tests

* Please write tests first before writing the code (TDD).

## Client side

* Use javascript to collect the statistics data and send them to the service (using a POST request).
* Make client code that will be hard to detect by blockers.
* Use best practices for the client side code.
* Use axios to make the requests to the service.
* Use local storage to store the data locally if the user has disabled cookies.
* Use service workers to collect the statistics data when the user is offline.
* Use reporting services to report errors and issues.
* Use best practices for the javascript code.

## Code quality

* Please use black to format the code.
* Please use isort to sort the imports.
* Please use flake8 to check the code for errors.
* Please use mypy to check the code for type errors.

## Documentation

* Please use mkdocs to document the project.
* Please use mkdocs-material to style the documentation.
* Please use mkdocs-static-i18n to add internationalization to the documentation.
* Please use mkdocs-git-revision-date-localized-plugin to add the last updated date to the documentation.
* Please use mkdocs-awesome-pages-plugin to add the pages to the documentation.
* Do not write comments in the code, but use good variable names and function names.

## Clickhouse

* Use clickhouse-connect to connect to the clickhouse database.
* Use correct engine for the tables.
* Make code to create the tables if they don't exist.
* Use correct data types for the columns.
* Make code to create the columns if they don't exist.
* Make sure to optimize the tables for the statistics data.

## Security

* Use API keys to authenticate the requests.
* Use rate limiting to avoid abuse (high load).